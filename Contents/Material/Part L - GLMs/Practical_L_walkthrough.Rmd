---
title: "Practical L"
author: "Gerko Vink"
date: "Statistical Programming in R"
output: html_document
---

We use the following packages:
```{r warning=FALSE, message=FALSE}
library(mice)
library(dplyr)
library(magrittr)
library(DAAG)
```

---

The following table shows numbers of occasions when inhibition (i.e., no flow of current across a membrane) occurred within 120 s, for different concentrations of the protein peptide-C. The outcome `yes` implies that inhibition has occurred.

    conc 0.1 0.5  1 10 20 30 50 70 80 100 150 
    no     7   1 10  9  2  9 13  1  1   4   3 
    yes    0   0  3  4  0  6  7  0  0   1   7

---

1. **Create this data in `R`**
```{r}
data <- data.frame(conc = c(.1, .5, 1, 10, 20, 30, 50, 70, 80, 100, 150),
                   no = c(7, 1, 10, 9, 2, 9, 13, 1, 1, 4, 3),
                   yes = c(0, 0, 3, 4, 0, 6, 7, 0, 0, 1 ,7)) 
data
```


---

2. **Add three new variables (columns) to the data**

- the margin of `no` and `yes` over the rows (i.e. `no` + `yes`)
- the proportion (`yes` over `margin`)
- the logodds

First, we create a function to calculate the logit (logodds):
```{r}
logit <- function(p) log(p / (1 - p))
```

Then we add the new columns
```{r}
data <- 
  data %>% 
  mutate(margin = no+yes,
         prop = yes / margin,
         logit = logit(prop))
```

---

3. **Inspect the expanded data. What do you see?**
```{r}
data
```

There are a lot of zero proportions, hence the $-\infty$ in the logit. You can fix this (at least the interpretation of the `logodds`) by adding a constant (usually 0.5) to all cells conform the empirical `logodds` [(see e.g. Cox and Snell 1989)](http://www.amazon.com/Analysis-Edition-Monographs-Statistics-Probability/dp/0412306204). 

---

4. **Add a new column where the log odds are calculated as:**
$$\log\text{odds} = \log\left(\frac{\text{yes} + 0.5}{\text{no} + 0.5}\right)$$
```{r}
logitCandS <- function(yes, no) log((yes + .5) / (no + .5))
data <- 
  data %>% 
  mutate(logitCS = logitCandS(yes, no))
data
```
We can now see that the $-\infty$ proportions are mostly gone

---

5. **Fit the model with `margin` as the weights**
```{r}
fit <- 
  data %$%
  glm(prop ~ conc, family=binomial, weights=margin)
```

---

6. **Look at the summary of the fitted object**
```{r}
summary(fit)
```

---

7. **Inspect the plots number 1 and 5 for `fit`**
```{r}
plot(fit, which = c(1, 5))
```

The data set is small, but case `11` stands out in the `Residuals vs. Leverage` plot

---

8. **`conc` is somewhat skewed. Run the model again with a log-transformation for `conc`.**
```{r echo = FALSE}
data$conc %>%
  density() %>%
  plot()
```


To apply this in the model directly:

```{r}
fit.log <- 
  data %$%
  glm(prop ~ I(log(conc)), family=binomial, weights=margin)
```

---

9. **Look at the summary of the fitted object again**
```{r}
summary(fit.log)
```
The logodds now depict the unit increase in `log(conc)`, instead of `conc`.

---

10. **Inspects the plots number 1 and 5 of the fitted object based on `log(conc)`.**
```{r}
plot(fit.log, which = c(1, 5))
```

Outliers are now less of an issue. This exercise demonstrates that data transformations may easily render our method more valid, but in exchange it makes our model interpretation more difficult. Parameters now have to be assessed in the `log(conc)` parameter space. 

---

11. **Use the `brandsma` data from package `mice` to fit a logistic regression model for `sex` based on `lpo` (Language Post Outcome).**
```{r}
brandsma.subset <- 
  brandsma %>%
  subset(!is.na(sex) & !is.na(lpo), select = c(sex, lpo))

fit <- 
  brandsma.subset %$%
  glm(sex ~ lpo, family=binomial(link='logit'))

fit %>%
  summary()
```

With every unit increase in `lpo`, the logodds of gender increases by `r coef(fit)[2]`. 

---

12. **Obtain confidence intervals for the parameter estimates.**
```{r}
confint(fit)
```

---

13. **Use the model parameters to predict the `sex` variable and compare your predictions to the observed `sex`. **
We can obtain predictions by using function `predict`. The default predictions are on the scale of the linear predictors; the alternative "response" is on the scale of the response variable. Thus for a default binomial model the default predictions are of log-odds (probabilities on logit scale) and type = "response" gives the predicted probabilities. 

To obtain the predicted logodds:
```{r}
pred.logodds <- 
  fit %>%
  predict()
head(pred.logodds)
```
and the predicted probabilities
```{r}
pred.prob <- 
  fit %>%
  predict(type = "response")
head(pred.prob)
```

We can then use the decision boundary `pred.prob > .5` to assign cases to `sex == 1` and the others to `sex == 0`. 
```{r}
pred <- numeric(length(pred.prob))
pred[pred.prob > .5] <- 1
pred <- unlist(pred)
```

To determine how many correct predictions we have, we can use
```{r}
obs <- 
  brandsma %>%
  subset(!is.na(sex) & !is.na(lpo), select = sex)

sum(pred == obs) #total correct
mean(pred == obs) #average correct
```
So we succesfully predict about half. That is not so good, because, based on chance alone, we would expect to successfully predict about half:
```{r}
set.seed(123)

table(obs) #Observed distribution

prop.obs <- #Observed proportions
  obs %>%
  table() %>%
  prop.table()

prop.obs
rand <- sample(c(0, 1), 3894, prob = c(.5123267, .4876733), replace = TRUE)
#random <- sample(0:1, nrow(obs), prob = prop.obs, replace = TRUE)

sum(rand == obs) #total correct
mean(rand == obs) #average correct
```

An alternative way to test our model efficiency is with `CVbinary()`:

```{r}
CVbinary(glm(sex ~ lpo, family=binomial(link='logit'), data = brandsma.subset))
```

---

The lesson here is that a significant paramater has no meaning if the substantive interpretation of the effect is ignored. There is almost no relation, whatsoever, there is just sufficient data to deem the influence of `lpo` on `sex` worthy of significance. 

---

14. **In the data set `minor.head.injury` (from package `DAAG`), obtain a logistic regression model relating `clinically.important.brain.injury` to all the other variables.**

Let us fit the model, predict `clinically.important.brain.injury` by all other variables in the data.
```{r}
fit <- glm(clinically.important.brain.injury ~ ., family=binomial, data=head.injury) 
summary(fit)
```


---

15. **Patients whose risk is sufficiently high will be sent for CT (computed tomography). Using a risk threshold of 0.025 (2.5%), turn the result into a decision rule for use of CT.**

A risk of 2.5% corresponds to the cutoff for a CT scan. This translates to a logit of $\log\left(\frac{.025}{1-.025}\right) = -3.663562$. In other words, any sum of variables that "lifts" the intercept above -3.66 would satisfy the cutoff. 

---

End of Practical